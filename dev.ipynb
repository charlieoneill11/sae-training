{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"openlifescienceai/medmcqa\", split=\"train\", streaming=False)\n",
    "\n",
    "rows_to_keep = 10_000\n",
    "\n",
    "# Only keep the first 10,000 rows\n",
    "ds = ds.select(range(rows_to_keep))\n",
    "\n",
    "def format_question_text(example):\n",
    "    \"\"\"\n",
    "    Transforms a dataset example into a formatted text string.\n",
    "    Args:\n",
    "        example: Dictionary containing the question data with keys:\n",
    "                'question', 'opa', 'opb', 'opc', 'opd', 'cop', 'exp'\n",
    "    Returns:\n",
    "        Dict with new 'text' key containing formatted string\n",
    "    \"\"\"\n",
    "    # Option keys in order\n",
    "    option_keys = ['opa', 'opb', 'opc', 'opd']\n",
    "\n",
    "    # Build the formatted string components\n",
    "    question = f\"{example['question']}\"\n",
    "    # Strip ':' from the end of the question\n",
    "    question = question.rstrip(':')\n",
    "    # Add a period to the end of the question\n",
    "    question = question + '?'\n",
    "    options = \"\\nThe options are:\\n\" + \"\\n\".join(example[key] for key in option_keys)\n",
    "\n",
    "    # Get correct option using the cop index\n",
    "    correct_idx = int(example['cop'])\n",
    "    correct_option = f\"\\nCorrect option: {example[option_keys[correct_idx]]}\"\n",
    "\n",
    "    # Add explanation if available\n",
    "    explanation = f\"\\nExplanation: {example['exp']}\" if 'exp' in example else \"\"\n",
    "    # Strip anything after and including 'Ref'\n",
    "    explanation = explanation.split('Ref')[0]\n",
    "\n",
    "    # Combine all components\n",
    "    formatted_text = f\"{question}{options}{correct_option}{explanation}\"\n",
    "\n",
    "    # Return dictionary with new text field\n",
    "    example['text'] = formatted_text\n",
    "    return example\n",
    "\n",
    "# Function to transform the entire dataset\n",
    "def transform_dataset(dataset):\n",
    "    \"\"\"\n",
    "    Applies the formatting transformation to the entire dataset.\n",
    "    Args:\n",
    "        dataset: Huggingface dataset object\n",
    "    Returns:\n",
    "        Transformed dataset with new 'text' column\n",
    "    \"\"\"\n",
    "    return dataset.map(\n",
    "        format_question_text,\n",
    "        desc=\"Formatting questions into text\",\n",
    "        num_proc=4  # Adjust based on your system\n",
    "    )\n",
    "\n",
    "transformed_ds = transform_dataset(ds)\n",
    "transformed_ds.to_pandas()\n",
    "# Drop all columns except text\n",
    "transformed_ds = transformed_ds.remove_columns([col for col in transformed_ds.column_names if col != 'text'])\n",
    "transformed_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from huggingface_hub import hf_hub_download\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "# Define the SAE model\n",
    "class JumpReLUSAE(nn.Module):\n",
    "    def __init__(self, d_model, d_sae):\n",
    "        super().__init__()\n",
    "        self.W_enc = nn.Parameter(torch.zeros(d_model, d_sae))\n",
    "        self.W_dec = nn.Parameter(torch.zeros(d_sae, d_model))\n",
    "        self.threshold = nn.Parameter(torch.zeros(d_sae))\n",
    "        self.b_enc = nn.Parameter(torch.zeros(d_sae))\n",
    "        self.b_dec = nn.Parameter(torch.zeros(d_model))\n",
    "\n",
    "        # Dimensions\n",
    "        self.d_model = d_model\n",
    "        self.d_sae = d_sae\n",
    "\n",
    "    def encode(self, input_acts):\n",
    "        pre_acts = input_acts @ self.W_enc + self.b_enc\n",
    "        mask = (pre_acts > self.threshold)\n",
    "        acts = mask * torch.nn.functional.relu(pre_acts)\n",
    "        return acts\n",
    "\n",
    "    def decode(self, acts):\n",
    "        return acts @ self.W_dec + self.b_dec\n",
    "\n",
    "    def forward(self, acts):\n",
    "        acts = self.encode(acts)\n",
    "        recon = self.decode(acts)\n",
    "        return recon\n",
    "\n",
    "width='16k'\n",
    "l0 = 71\n",
    "layer = 20\n",
    "\n",
    "# Load the SAE model\n",
    "path_to_params = hf_hub_download(\n",
    "    repo_id=\"google/gemma-scope-2b-pt-res\",\n",
    "    filename=f\"layer_{layer}/width_{width}/average_l0_{l0}/params.npz\",\n",
    "    force_download=False,\n",
    ")\n",
    "\n",
    "params = np.load(path_to_params)\n",
    "pt_params = {k: torch.from_numpy(v).cpu() for k, v in params.items()}\n",
    "\n",
    "# Initialize and load the SAE model\n",
    "sae = JumpReLUSAE(params['W_enc'].shape[0], params['W_enc'].shape[1])\n",
    "sae.load_state_dict(pt_params)\n",
    "sae = sae.cpu()\n",
    "\n",
    "# Load your data from Hugging Face\n",
    "repo_id = \"charlieoneill/gemma-medicine-sae\"  # Replace with your repo\n",
    "\n",
    "# Download the activation tensor and dataset\n",
    "api = HfApi()\n",
    "activation_file = hf_hub_download(repo_id=repo_id, filename=\"10000_128.pt\")\n",
    "\n",
    "# Load the tensors\n",
    "activations = torch.load(activation_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae.d_sae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to GPU\n",
    "activations = activations.cpu()\n",
    "\n",
    "# Process a batch of 32\n",
    "# batch_size = 1\n",
    "# batch_acts = activations[:batch_size]\n",
    "batch_acts = activations[4].unsqueeze(0)\n",
    "\n",
    "# Run through SAE\n",
    "with torch.no_grad():\n",
    "    recon = sae(batch_acts)\n",
    "\n",
    "# Calculate variance explained\n",
    "variance_explained = 1 - torch.mean((recon[:, 1:] - batch_acts[:, 1:].to(torch.float32)) **2) / (batch_acts[:, 1:].to(torch.float32).var())\n",
    "\n",
    "# Calculate L0 sparsity\n",
    "with torch.no_grad():\n",
    "    encoded = sae.encode(batch_acts)\n",
    "    l0_sparsity = (encoded > 0).float().mean()\n",
    "\n",
    "print(f\"Variance explained: {variance_explained.item():.4f}\")\n",
    "print(f\"L0 sparsity: {l0_sparsity.item():.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_act = activations[4].unsqueeze(0)\n",
    "print(target_act.shape)\n",
    "\n",
    "sae_acts = sae.encode(target_act.to(torch.float32))\n",
    "recon = sae.decode(sae_acts)\n",
    "\n",
    "print(sae_acts.shape, recon.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print MSE loss between target_act and recon\n",
    "loss = torch.mean((recon[:, 1:] - target_act[:, 1:].to(torch.float32)) **2)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - torch.mean((recon[:, 1:] - target_act[:, 1:].to(torch.float32)) **2) / (target_act[:, 1:].to(torch.float32).var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sae_acts > 1).sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# values, inds = sae_acts.max(-1)\n",
    "\n",
    "# inds, inds.shape\n",
    "\n",
    "# First flatten the sequence dimension, but exclude first position\n",
    "flat_acts = sae_acts[:, 1:, :].reshape(sae_acts.shape[0], -1)  # Note the 1: slice\n",
    "\n",
    "# Get top 10 values and indices\n",
    "top_values, top_indices = torch.topk(flat_acts, k=10, dim=-1)\n",
    "\n",
    "# Convert flat indices back to (seq_pos, feature) pairs\n",
    "# Add 1 to seq_pos since we excluded the first position\n",
    "seq_pos = (top_indices // sae_acts.shape[-1]) + 1  # add 1 to account for skipped first position\n",
    "feature_ids = top_indices % sae_acts.shape[-1]\n",
    "\n",
    "# Print results\n",
    "for i in range(10):\n",
    "    print(f\"Position {seq_pos[0][i]}, Feature {feature_ids[0][i]}: Activation {top_values[0][i]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transformed_ds[4]['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "html_template = \"https://neuronpedia.org/{}/{}/{}?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
    "\n",
    "def get_dashboard_html(sae_release = \"gemma-2-2b\", sae_id=f\"{layer}-gemmascope-res-{width}\", feature_idx=0):\n",
    "    return html_template.format(sae_release, sae_id, feature_idx)\n",
    "\n",
    "html = get_dashboard_html(sae_release = \"gemma-2-2b\", sae_id=f\"{layer}-gemmascope-res-{width}\", feature_idx=19451)\n",
    "IFrame(html, width=1200, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "#url = \"https://www.neuronpedia.org/api/explanation/export?modelId=gpt2-small&saeId=7-res-jb\"\n",
    "url = f\"https://www.neuronpedia.org/api/explanation/export?modelId=gemma-2-2b&saeId={layer}-gemmascope-res-{width}\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "response = requests.get(url, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# convert to pandas\n",
    "data = response.json()\n",
    "explanations_df = pd.DataFrame(data)\n",
    "# rename index to \"feature\"\n",
    "explanations_df.rename(columns={\"index\": \"feature\"}, inplace=True)\n",
    "# explanations_df[\"feature\"] = explanations_df[\"feature\"].astype(int)\n",
    "explanations_df[\"description\"] = explanations_df[\"description\"].apply(\n",
    "    lambda x: x.lower()\n",
    ")\n",
    "explanations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "values, feature_ids = sae_acts.max(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activating_features = list(set(feature_ids[0].cpu().numpy()))\n",
    "\n",
    "# Get the explanations for these features\n",
    "explanations_df.loc[activating_features]\n",
    "\n",
    "# Print the feature and explanation, one by one\n",
    "for feature in activating_features:\n",
    "    print(f\"Feature {feature}:\")\n",
    "    print(explanations_df.loc[feature][\"description\"])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the dashboard for this feature.\n",
    "html = get_dashboard_html(\n",
    "    sae_release=\"gpt2-small\",\n",
    "    sae_id=\"7-res-jb\",\n",
    "    feature_idx=bible_features.feature.values[0],\n",
    ")\n",
    "IFrame(html, width=1200, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sae-training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
