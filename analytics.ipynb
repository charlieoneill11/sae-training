{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from dictionary import JumpReluAutoEncoder\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing ours vs GemmaScope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_path = 'trained/ae.pt'\n",
    "config_path = 'trained/config.json'\n",
    "\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "d_model = config['trainer']['activation_dim']\n",
    "d_sae = config['trainer']['dict_size']\n",
    "device = 'cpu'\n",
    "\n",
    "# Create the JumpReluAutoEncoder instance\n",
    "ae = JumpReluAutoEncoder(activation_dim=d_model, dict_size=d_sae, device=device)\n",
    "\n",
    "# Load the state dictionary from the saved checkpoint\n",
    "checkpoint = torch.load(ae_path, map_location=device)\n",
    "ae.load_state_dict(checkpoint)\n",
    "\n",
    "# The autoencoder is now ready for inference or further fine-tuning.\n",
    "print(\"Successfully loaded JumpRelu SAE from checkpoint.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from huggingface_hub import hf_hub_download\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "# Define the SAE model\n",
    "class JumpReLUGemma(nn.Module):\n",
    "    def __init__(self, d_model, d_sae):\n",
    "        super().__init__()\n",
    "        self.W_enc = nn.Parameter(torch.zeros(d_model, d_sae))\n",
    "        self.W_dec = nn.Parameter(torch.zeros(d_sae, d_model))\n",
    "        self.threshold = nn.Parameter(torch.zeros(d_sae))\n",
    "        self.b_enc = nn.Parameter(torch.zeros(d_sae))\n",
    "        self.b_dec = nn.Parameter(torch.zeros(d_model))\n",
    "\n",
    "        # Dimensions\n",
    "        self.d_model = d_model\n",
    "        self.d_sae = d_sae\n",
    "\n",
    "    def encode(self, input_acts):\n",
    "        pre_acts = input_acts @ self.W_enc + self.b_enc\n",
    "        mask = (pre_acts > self.threshold)\n",
    "        acts = mask * torch.nn.functional.relu(pre_acts)\n",
    "        return acts\n",
    "\n",
    "    def decode(self, acts):\n",
    "        return acts @ self.W_dec + self.b_dec\n",
    "\n",
    "    def forward(self, acts):\n",
    "        acts = self.encode(acts)\n",
    "        recon = self.decode(acts)\n",
    "        return recon\n",
    "\n",
    "width='16k'\n",
    "l0 = 71\n",
    "layer = 20\n",
    "\n",
    "# Load the SAE model\n",
    "path_to_params = hf_hub_download(\n",
    "    repo_id=\"google/gemma-scope-2b-pt-res\",\n",
    "    filename=f\"layer_{layer}/width_{width}/average_l0_{l0}/params.npz\",\n",
    "    force_download=False,\n",
    ")\n",
    "\n",
    "params = np.load(path_to_params)\n",
    "pt_params = {k: torch.from_numpy(v).cpu() for k, v in params.items()}\n",
    "\n",
    "# Initialize and load the SAE model\n",
    "gemma = JumpReLUGemma(params['W_enc'].shape[0], params['W_enc'].shape[1])\n",
    "gemma.load_state_dict(pt_params)\n",
    "gemma = gemma.cpu()\n",
    "\n",
    "# Load your data from Hugging Face\n",
    "repo_id = \"charlieoneill/gemma-medicine-sae\"  # Replace with your repo\n",
    "\n",
    "# Download the activation tensor and dataset\n",
    "api = HfApi()\n",
    "activation_file = hf_hub_download(repo_id=repo_id, filename=\"10000_128.pt\")\n",
    "\n",
    "# Load the tensors\n",
    "activations = torch.load(activation_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def efficient_cosine_similarity(matrix1, matrix2):\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between each row of matrix1 and each row of matrix2.\n",
    "    \n",
    "    Args:\n",
    "        matrix1: numpy array of shape (n_features_1, d_model)\n",
    "        matrix2: numpy array of shape (n_features_2, d_model)\n",
    "        \n",
    "    Returns:\n",
    "        similarity_matrix: numpy array of shape (n_features_1, n_features_2)\n",
    "    \"\"\"\n",
    "    # Normalize the matrices along rows\n",
    "    # Using einsum for efficient norm calculation\n",
    "    norm1 = np.sqrt(np.einsum('ij,ij->i', matrix1, matrix1))\n",
    "    norm2 = np.sqrt(np.einsum('ij,ij->i', matrix2, matrix2))\n",
    "    \n",
    "    # Add extra dimension for broadcasting\n",
    "    norm1 = norm1[:, np.newaxis]\n",
    "    norm2 = norm2[:, np.newaxis] #norm2[np.newaxis, :]\n",
    "    \n",
    "    # Normalize matrices\n",
    "    matrix1_normalized = matrix1 / norm1\n",
    "    matrix2_normalized = matrix2 / norm2\n",
    "    \n",
    "    # Calculate dot product of normalized matrices\n",
    "    # This gives us cosine similarity\n",
    "    similarity_matrix = np.dot(matrix1_normalized, matrix2_normalized.T)\n",
    "    \n",
    "    return similarity_matrix\n",
    "\n",
    "enc_ae = ae.W_enc.detach().cpu().numpy() # (2304 x 16384) = (d_model x d_sae)\n",
    "enc_gemma = gemma.W_enc.detach().cpu().numpy() # (2304 x 16384) = (d_model x d_sae)\n",
    "dec_ae = ae.W_dec.detach().cpu().numpy() # (16384 x 2304) = (d_sae x d_model)\n",
    "dec_gemma = gemma.W_dec.detach().cpu().numpy() # (16384 x 2304) = (d_sae x d_model)\n",
    "\n",
    "assert enc_ae.shape == enc_gemma.shape\n",
    "assert dec_ae.shape == dec_gemma.shape\n",
    "\n",
    "similarity_matrix = efficient_cosine_similarity(dec_ae, dec_gemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "# Get maximum similarity for each feature in matrix1 (max along each row)\n",
    "max_similarities = np.max(similarity_matrix, axis=1)\n",
    "\n",
    "# Create histogram\n",
    "fig = go.Figure(data=[go.Histogram(\n",
    "    x=max_similarities,\n",
    "    nbinsx=50,\n",
    "    name='Max Cosine Similarities',\n",
    "    opacity=0.75\n",
    ")])\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    xaxis_title='Maximum Cosine Similarity',\n",
    "    yaxis_title='Count of Features from IrisSAE',\n",
    "    bargap=0.1,\n",
    "    showlegend=False,\n",
    ")\n",
    "\n",
    "# Add annotations explaining the meaning\n",
    "fig.add_annotation(\n",
    "    text='Each data point represents a feature from IrisSAE,\\nshowing its highest similarity to any feature in GemmaScopeSAE',\n",
    "    xref='paper', yref='paper',\n",
    "    x=0.5, y=1.15,\n",
    "    showarrow=False,\n",
    "    font=dict(size=12)\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "import seaborn as sns\n",
    "\n",
    "def compute_cosine_similarities(matrix1, matrix2):\n",
    "    \"\"\"Compute cosine similarity between corresponding vectors in two matrices.\"\"\"\n",
    "    norm1 = np.linalg.norm(matrix1, axis=1, keepdims=True)\n",
    "    norm2 = np.linalg.norm(matrix2, axis=1, keepdims=True)\n",
    "    matrix1_norm = matrix1 / norm1\n",
    "    matrix2_norm = matrix2 / norm2\n",
    "    return np.dot(matrix1_norm, matrix2_norm.T)\n",
    "\n",
    "def create_alignment_plot(enc_ae, enc_gemma, dec_ae, dec_gemma):\n",
    "    # Transpose encoder weights to match dimensions\n",
    "    enc_ae = enc_ae.T  # Now (16384 x 2304)\n",
    "    enc_gemma = enc_gemma.T  # Now (16384 x 2304)\n",
    "    \n",
    "    # Compute similarities\n",
    "    enc_similarities = compute_cosine_similarities(enc_ae, enc_gemma)\n",
    "    dec_similarities = compute_cosine_similarities(dec_ae, dec_gemma)\n",
    "    \n",
    "    # Get Hungarian matching\n",
    "    average_similarities = (enc_similarities + dec_similarities) / 2\n",
    "    row_ind, col_ind = linear_sum_assignment(-average_similarities)\n",
    "    \n",
    "    # Get matched similarities\n",
    "    matched_enc_sims = enc_similarities[row_ind, col_ind]\n",
    "    matched_dec_sims = dec_similarities[row_ind, col_ind]\n",
    "    \n",
    "    # Determine which matches are \"equal\"\n",
    "    is_shared = (matched_enc_sims >= 0.7) & (matched_dec_sims >= 0.7)\n",
    "    \n",
    "    # Create the plot\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    \n",
    "    # Define the geometry for the scatter plot and marginals\n",
    "    gs = plt.GridSpec(3, 3)\n",
    "    ax_scatter = fig.add_subplot(gs[1:, :-1])\n",
    "    ax_hist_x = fig.add_subplot(gs[0, :-1])\n",
    "    ax_hist_y = fig.add_subplot(gs[1:, -1])\n",
    "    \n",
    "    # Plot points\n",
    "    ax_scatter.scatter(matched_dec_sims[~is_shared], matched_enc_sims[~is_shared], \n",
    "                      c='blue', s=1, alpha=0.5, label='Different')\n",
    "    ax_scatter.scatter(matched_dec_sims[is_shared], matched_enc_sims[is_shared], \n",
    "                      c='orange', s=1, alpha=0.5, label='Equal')\n",
    "    \n",
    "    # Add contours for each category\n",
    "    for mask, color in [(~is_shared, 'blue'), (is_shared, 'orange')]:\n",
    "        if np.sum(mask) > 10:\n",
    "            x = matched_dec_sims[mask]\n",
    "            y = matched_enc_sims[mask]\n",
    "            xy = np.vstack([x, y])\n",
    "            kde = gaussian_kde(xy)\n",
    "            \n",
    "            # Create a regular grid to evaluate kde\n",
    "            xgrid = np.linspace(0, 1, 100)\n",
    "            ygrid = np.linspace(0, 1, 100)\n",
    "            Xgrid, Ygrid = np.meshgrid(xgrid, ygrid)\n",
    "            Z = kde.evaluate(np.vstack([Xgrid.ravel(), Ygrid.ravel()])).reshape(Xgrid.shape)\n",
    "            \n",
    "            # Plot contours\n",
    "            ax_scatter.contour(Xgrid, Ygrid, Z, levels=7, colors=color, alpha=0.5)\n",
    "    \n",
    "    # Add marginal distributions\n",
    "    sns.kdeplot(data=matched_dec_sims, ax=ax_hist_x, color='gray')\n",
    "    sns.kdeplot(data=matched_enc_sims, ax=ax_hist_y, color='gray', vertical=True)\n",
    "    \n",
    "    # Adjust layouts\n",
    "    ax_scatter.set_xlabel('Decoder alignment')\n",
    "    ax_scatter.set_ylabel('Encoder alignment')\n",
    "    ax_scatter.set_xlim(0, 1)\n",
    "    ax_scatter.set_ylim(0, 1)\n",
    "    ax_scatter.legend()\n",
    "    ax_scatter.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Remove labels from marginal plots\n",
    "    ax_hist_x.set_xticks([])\n",
    "    ax_hist_y.set_yticks([])\n",
    "    \n",
    "    plt.suptitle('Encoder-decoder Hungarian matching')\n",
    "    \n",
    "    # Calculate and print statistics\n",
    "    total_latents = len(matched_enc_sims)\n",
    "    shared_latents = np.sum(is_shared)\n",
    "    print(f\"Total latents: {total_latents}\")\n",
    "    print(f\"Shared latents (cos sim ≥ 0.7): {shared_latents}\")\n",
    "    print(f\"Fraction shared: {shared_latents/total_latents:.3f}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Run the analysis\n",
    "fig = create_alignment_plot(enc_ae, enc_gemma, dec_ae, dec_gemma)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    target_act = activations[:128]\n",
    "    print(\"Target act shape: \",target_act.shape)\n",
    "\n",
    "    sae_acts = ae.encode(target_act.to(torch.float32))\n",
    "    print(\"SAE acts shape: \", sae_acts.shape)\n",
    "    recon = ae.decode(sae_acts)\n",
    "    print(\"Recon shape: \", recon.shape)\n",
    "\n",
    "    # Print MSE loss between target_act and recon\n",
    "    loss = torch.mean((recon[:, 1:] - target_act[:, 1:].to(torch.float32)) **2)\n",
    "    print(\"Loss: \", loss.item())\n",
    "\n",
    "    variance_explained = 1 - torch.mean((recon[:, 1:] - target_act[:, 1:].to(torch.float32)) **2) / (target_act[:, 1:].to(torch.float32).var())\n",
    "    print(\"Variance explained: \", variance_explained.item())\n",
    "\n",
    "    l0 = (sae_acts > 0).sum(-1).float().mean().item()\n",
    "    print(\"L0: \", l0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # Initialize lists to store statistics for each batch\n",
    "    losses = []\n",
    "    variances_explained = []\n",
    "    l0_stats = []\n",
    "    \n",
    "    # Calculate number of batches\n",
    "    batch_size = 128\n",
    "    total_samples = activations.shape[0]\n",
    "    num_batches = (total_samples + batch_size - 1) // batch_size  # Ceiling division\n",
    "    \n",
    "    for i in tqdm(range(num_batches)):\n",
    "        # Get batch slice\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, total_samples)\n",
    "        target_act = activations[start_idx:end_idx]\n",
    "        \n",
    "        # Forward pass\n",
    "        sae_acts = ae.encode(target_act.to(torch.float32))\n",
    "        recon = ae.decode(sae_acts)\n",
    "        \n",
    "        # Calculate statistics\n",
    "        # MSE Loss\n",
    "        loss = torch.mean((recon[:, 1:] - target_act[:, 1:].to(torch.float32)) ** 2)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # Variance explained\n",
    "        var_exp = 1 - torch.mean((recon[:, 1:] - target_act[:, 1:].to(torch.float32)) ** 2) / (target_act[:, 1:].to(torch.float32).var())\n",
    "        variances_explained.append(var_exp.item())\n",
    "        \n",
    "        # L0 statistic\n",
    "        l0 = (sae_acts > 0).sum(-1).float().mean().item()\n",
    "        l0_stats.append(l0)\n",
    "    \n",
    "    # Calculate final statistics\n",
    "    mean_loss = np.mean(losses)\n",
    "    mean_variance_explained = np.mean(variances_explained)\n",
    "    mean_l0 = np.mean(l0_stats)\n",
    "    \n",
    "    # Calculate standard deviations\n",
    "    std_loss = np.std(losses)\n",
    "    std_variance_explained = np.std(variances_explained)\n",
    "    std_l0 = np.std(l0_stats)\n",
    "    \n",
    "    # Print final results\n",
    "    print(f\"Final statistics over {total_samples} samples:\")\n",
    "    print(f\"Mean Loss: {mean_loss:.6f} ± {std_loss:.6f}\")\n",
    "    print(f\"Mean Variance Explained: {mean_variance_explained:.6f} ± {std_variance_explained:.6f}\")\n",
    "    print(f\"Mean L0: {mean_l0:.2f} ± {std_l0:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # Initialize lists for both SAEs\n",
    "    stats = {\n",
    "        'ae': {'losses': [], 'variances_explained': [], 'l0_stats': []},\n",
    "        'gemma': {'losses': [], 'variances_explained': [], 'l0_stats': []}\n",
    "    }\n",
    "    \n",
    "    # Calculate number of batches\n",
    "    batch_size = 128\n",
    "    total_samples = activations.shape[0]\n",
    "    num_batches = (total_samples + batch_size - 1) // batch_size\n",
    "    \n",
    "    for i in tqdm(range(num_batches)):\n",
    "        # Get batch slice\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, total_samples)\n",
    "        target_act = activations[start_idx:end_idx]\n",
    "        target_act_float = target_act.to(torch.float32)\n",
    "        \n",
    "        # Process both SAEs\n",
    "        for model_name, model in [('ae', ae), ('gemma', gemma)]:\n",
    "            # Forward pass\n",
    "            sae_acts = model.encode(target_act_float)\n",
    "            recon = model.decode(sae_acts)\n",
    "            \n",
    "            # Calculate statistics\n",
    "            loss = torch.mean((recon[:, 1:] - target_act_float[:, 1:]) ** 2)\n",
    "            stats[model_name]['losses'].append(loss.item())\n",
    "            \n",
    "            var_exp = 1 - torch.mean((recon[:, 1:] - target_act_float[:, 1:]) ** 2) / (target_act_float[:, 1:].var())\n",
    "            stats[model_name]['variances_explained'].append(var_exp.item())\n",
    "            \n",
    "            l0 = (sae_acts > 0).sum(-1).float().mean().item()\n",
    "            stats[model_name]['l0_stats'].append(l0)\n",
    "    \n",
    "    # Print final results for both models\n",
    "    print(\"Final statistics over {} samples:\".format(total_samples))\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for model_name in ['ae', 'gemma']:\n",
    "        mean_loss = np.mean(stats[model_name]['losses'])\n",
    "        mean_variance_explained = np.mean(stats[model_name]['variances_explained'])\n",
    "        mean_l0 = np.mean(stats[model_name]['l0_stats'])\n",
    "        \n",
    "        std_loss = np.std(stats[model_name]['losses'])\n",
    "        std_variance_explained = np.std(stats[model_name]['variances_explained'])\n",
    "        std_l0 = np.std(stats[model_name]['l0_stats'])\n",
    "        \n",
    "        print(f\"\\n{model_name.upper()} Statistics:\")\n",
    "        print(f\"Mean Loss: {mean_loss:.6f} ± {std_loss:.6f}\")\n",
    "        print(f\"Mean Variance Explained: {mean_variance_explained:.6f} ± {std_variance_explained:.6f}\")\n",
    "        print(f\"Mean L0: {mean_l0:.2f} ± {std_l0:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
