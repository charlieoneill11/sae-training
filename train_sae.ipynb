{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import torch\n",
    "from jumprelu_sae import JumpReLUSAE  # Make sure jumprelu_sae.py is in your PYTHONPATH or same directory\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import hf_hub_download\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Automatically select between CUDA and CPU.\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load the activations.\n",
    "    # (Assumes a file 'activations.pt' containing a tensor of shape [10000, 128, 2304])\n",
    "    try:\n",
    "        # Load your data from Hugging Face\n",
    "        repo_id = \"charlieoneill/gemma-medicine-sae\"  # Replace with your repo\n",
    "\n",
    "        # Download the activation tensor and dataset\n",
    "        api = HfApi()\n",
    "        activation_file = hf_hub_download(repo_id=repo_id, filename=\"10000_128.pt\")\n",
    "\n",
    "        # Load the tensors\n",
    "        activations = torch.load(activation_file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading activations: {e}\")\n",
    "        return\n",
    "\n",
    "    # Instantiate the SAE.\n",
    "    # Use the last dimension of activations as d_model, and choose a latent dimension (d_sae).\n",
    "    d_model = activations.shape[-1]  # e.g. 2304\n",
    "    d_sae = 16384  # Example latent dimension; adjust as needed.\n",
    "    sae = JumpReLUSAE(d_model=d_model, d_sae=d_sae, sparsity_coeff=100.0)\n",
    "    sae.to(device)\n",
    "\n",
    "    # Define training hyperparameters.\n",
    "    batch_size = 128     # Mini-batch size (each token is treated as a separate example)\n",
    "    log_freq = 10        # How often to log training statistics\n",
    "    lr = 1e-3             # Base learning rate\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "\n",
    "    # Train the model on the activations.\n",
    "    # The optimize_on_activations method expects a tensor of shape (N, seq_len, d_model)\n",
    "    # and will flatten the sequence dimension into the batch.\n",
    "    data_log = sae.optimize_on_activations(\n",
    "        activations,\n",
    "        batch_size=batch_size,\n",
    "        epochs=10,  # Changed from steps=steps to epochs=10 (or your desired number of epochs)\n",
    "        log_freq=log_freq,\n",
    "        lr=lr\n",
    "    )\n",
    "\n",
    "    print(\"Training completed.\")\n",
    "\n",
    "    # Print final statistics from the training log.\n",
    "    if data_log:\n",
    "        final_stats = data_log[-1]\n",
    "        print(\"\\nFinal training statistics:\")\n",
    "        for key, value in final_stats.items():\n",
    "            print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
