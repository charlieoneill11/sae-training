{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import torch\n",
    "from jumprelu_sae import JumpReLUSAE  # Make sure jumprelu_sae.py is in your PYTHONPATH or same directory\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import hf_hub_download\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Automatically select between CUDA and CPU.\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load the activations.\n",
    "    # (Assumes a file 'activations.pt' containing a tensor of shape [10000, 128, 2304])\n",
    "    try:\n",
    "        # Load your data from Hugging Face\n",
    "        repo_id = \"charlieoneill/gemma-medicine-sae\"  # Replace with your repo\n",
    "\n",
    "        # Download the activation tensor and dataset\n",
    "        api = HfApi()\n",
    "        activation_file = hf_hub_download(repo_id=repo_id, filename=\"10000_128.pt\")\n",
    "\n",
    "        # Load the tensors\n",
    "        activations = torch.load(activation_file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading activations: {e}\")\n",
    "        return\n",
    "\n",
    "    # Instantiate the SAE.\n",
    "    # Use the last dimension of activations as d_model, and choose a latent dimension (d_sae).\n",
    "    d_model = activations.shape[-1]  # e.g. 2304\n",
    "    d_sae = 16384  # Example latent dimension; adjust as needed.\n",
    "    sae = JumpReLUSAE(d_model=d_model, d_sae=d_sae, sparsity_coeff=100.0)\n",
    "    sae.to(device)\n",
    "\n",
    "    # Define training hyperparameters.\n",
    "    batch_size = 1024     # Mini-batch size (each token is treated as a separate example)\n",
    "    log_freq = 10        # How often to log training statistics\n",
    "    lr = 1e-3             # Base learning rate\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "\n",
    "    # Train the model on the activations.\n",
    "    # The optimize_on_activations method expects a tensor of shape (N, seq_len, d_model)\n",
    "    # and will flatten the sequence dimension into the batch.\n",
    "    data_log = sae.optimize_on_activations(\n",
    "        activations,\n",
    "        batch_size=batch_size,\n",
    "        epochs=10,  # Changed from steps=steps to epochs=10 (or your desired number of epochs)\n",
    "        log_freq=log_freq,\n",
    "        lr=lr\n",
    "    )\n",
    "\n",
    "    print(\"Training completed.\")\n",
    "\n",
    "    # Print final statistics from the training log.\n",
    "    if data_log:\n",
    "        final_stats = data_log[-1]\n",
    "        print(\"\\nFinal training statistics:\")\n",
    "        for key, value in final_stats.items():\n",
    "            print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wg/cdt_cw_5265_z_gwnxlf0tv80000gn/T/ipykernel_93208/2692151815.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  activations = torch.load(activation_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Total steps: 12500 (steps per epoch: 1250)\n",
      "Epoch 0/10, Step 0: lr=0.001000, recon_loss=79.583023, sparsity_loss=7741.051758, frac_active=0.380379\n",
      "Epoch 0/10, Step 10: lr=0.001000, recon_loss=48.909981, sparsity_loss=344.666992, frac_active=0.021176\n",
      "Epoch 0/10, Step 20: lr=0.001000, recon_loss=21.469131, sparsity_loss=441.949219, frac_active=0.027181\n",
      "Epoch 0/10, Step 30: lr=0.001000, recon_loss=16.837908, sparsity_loss=467.939453, frac_active=0.028789\n",
      "Epoch 0/10, Step 40: lr=0.001000, recon_loss=14.451908, sparsity_loss=501.421875, frac_active=0.030780\n",
      "Epoch 0/10, Step 50: lr=0.001000, recon_loss=12.341775, sparsity_loss=560.317383, frac_active=0.034555\n",
      "Epoch 0/10, Step 60: lr=0.001000, recon_loss=10.886212, sparsity_loss=610.457031, frac_active=0.037544\n",
      "Epoch 0/10, Step 70: lr=0.001000, recon_loss=9.977271, sparsity_loss=668.626953, frac_active=0.041090\n",
      "Epoch 0/10, Step 80: lr=0.001000, recon_loss=9.330170, sparsity_loss=703.653320, frac_active=0.043200\n",
      "Epoch 0/10, Step 90: lr=0.001000, recon_loss=8.653893, sparsity_loss=752.834961, frac_active=0.046206\n",
      "Epoch 0/10, Step 100: lr=0.001000, recon_loss=8.093732, sparsity_loss=787.259766, frac_active=0.048251\n",
      "Epoch 0/10, Step 110: lr=0.001000, recon_loss=7.236014, sparsity_loss=804.850586, frac_active=0.049353\n",
      "Epoch 0/10, Step 120: lr=0.001000, recon_loss=7.013887, sparsity_loss=853.149414, frac_active=0.052250\n",
      "Epoch 0/10, Step 130: lr=0.001000, recon_loss=6.836498, sparsity_loss=848.403320, frac_active=0.051861\n",
      "Epoch 0/10, Step 140: lr=0.001000, recon_loss=6.339196, sparsity_loss=873.368164, frac_active=0.053467\n",
      "Epoch 0/10, Step 150: lr=0.001000, recon_loss=5.957955, sparsity_loss=916.795898, frac_active=0.056115\n",
      "Epoch 0/10, Step 160: lr=0.001000, recon_loss=5.850845, sparsity_loss=942.916016, frac_active=0.057743\n",
      "Epoch 0/10, Step 170: lr=0.001000, recon_loss=5.642419, sparsity_loss=973.168945, frac_active=0.059497\n",
      "Epoch 0/10, Step 180: lr=0.001000, recon_loss=5.146655, sparsity_loss=980.175781, frac_active=0.059933\n",
      "Epoch 0/10, Step 190: lr=0.001000, recon_loss=5.068601, sparsity_loss=1003.472656, frac_active=0.061329\n",
      "Epoch 0/10, Step 200: lr=0.001000, recon_loss=4.803387, sparsity_loss=1011.666016, frac_active=0.061790\n",
      "Epoch 0/10, Step 210: lr=0.001000, recon_loss=4.862680, sparsity_loss=1025.286133, frac_active=0.062731\n",
      "Epoch 0/10, Step 220: lr=0.001000, recon_loss=4.457196, sparsity_loss=1044.965820, frac_active=0.063882\n",
      "Epoch 0/10, Step 230: lr=0.001000, recon_loss=4.422743, sparsity_loss=1063.379883, frac_active=0.064986\n",
      "Epoch 0/10, Step 240: lr=0.001000, recon_loss=4.202344, sparsity_loss=1087.681641, frac_active=0.066460\n",
      "Epoch 0/10, Step 250: lr=0.001000, recon_loss=4.089299, sparsity_loss=1083.412109, frac_active=0.066093\n",
      "Epoch 0/10, Step 260: lr=0.001000, recon_loss=4.019867, sparsity_loss=1095.397461, frac_active=0.066937\n",
      "Epoch 0/10, Step 270: lr=0.001000, recon_loss=3.843364, sparsity_loss=1095.921875, frac_active=0.066906\n",
      "Epoch 0/10, Step 280: lr=0.001000, recon_loss=3.727419, sparsity_loss=1116.280273, frac_active=0.068234\n",
      "Epoch 0/10, Step 290: lr=0.001000, recon_loss=3.675617, sparsity_loss=1139.236328, frac_active=0.069601\n",
      "Epoch 0/10, Step 300: lr=0.001000, recon_loss=3.448299, sparsity_loss=1147.452148, frac_active=0.070120\n",
      "Epoch 0/10, Step 310: lr=0.001000, recon_loss=3.808824, sparsity_loss=1119.793945, frac_active=0.068018\n",
      "Epoch 0/10, Step 320: lr=0.001000, recon_loss=3.378357, sparsity_loss=1146.549805, frac_active=0.070044\n",
      "Epoch 0/10, Step 330: lr=0.001000, recon_loss=3.186680, sparsity_loss=1156.463867, frac_active=0.070690\n",
      "Epoch 0/10, Step 340: lr=0.001000, recon_loss=3.061282, sparsity_loss=1184.687500, frac_active=0.072312\n",
      "Epoch 0/10, Step 350: lr=0.001000, recon_loss=3.207165, sparsity_loss=1175.761719, frac_active=0.071818\n",
      "Epoch 0/10, Step 360: lr=0.001000, recon_loss=3.034445, sparsity_loss=1188.523438, frac_active=0.072625\n",
      "Epoch 0/10, Step 370: lr=0.001000, recon_loss=2.925456, sparsity_loss=1214.810547, frac_active=0.074211\n",
      "Epoch 0/10, Step 380: lr=0.001000, recon_loss=3.063815, sparsity_loss=1199.983398, frac_active=0.073173\n",
      "Epoch 0/10, Step 390: lr=0.001000, recon_loss=2.904993, sparsity_loss=1208.372070, frac_active=0.073830\n",
      "Epoch 0/10, Step 400: lr=0.001000, recon_loss=2.994959, sparsity_loss=1197.650391, frac_active=0.072924\n",
      "Epoch 0/10, Step 410: lr=0.001000, recon_loss=2.742701, sparsity_loss=1220.920898, frac_active=0.074580\n",
      "Epoch 0/10, Step 420: lr=0.001000, recon_loss=2.776292, sparsity_loss=1244.809570, frac_active=0.076062\n",
      "Epoch 0/10, Step 430: lr=0.001000, recon_loss=2.703614, sparsity_loss=1254.090820, frac_active=0.076617\n",
      "Epoch 0/10, Step 440: lr=0.001000, recon_loss=2.584384, sparsity_loss=1251.682617, frac_active=0.076395\n",
      "Epoch 0/10, Step 450: lr=0.001000, recon_loss=2.729478, sparsity_loss=1252.087891, frac_active=0.076356\n",
      "Epoch 0/10, Step 460: lr=0.001000, recon_loss=2.427530, sparsity_loss=1257.068359, frac_active=0.076820\n",
      "Epoch 0/10, Step 470: lr=0.001000, recon_loss=2.451319, sparsity_loss=1271.072266, frac_active=0.077595\n",
      "Epoch 0/10, Step 480: lr=0.001000, recon_loss=2.823262, sparsity_loss=1280.376953, frac_active=0.077843\n",
      "Epoch 0/10, Step 490: lr=0.001000, recon_loss=2.441153, sparsity_loss=1270.055664, frac_active=0.077687\n",
      "Epoch 0/10, Step 500: lr=0.001000, recon_loss=2.244690, sparsity_loss=1281.536133, frac_active=0.078340\n",
      "Epoch 0/10, Step 510: lr=0.001000, recon_loss=2.531983, sparsity_loss=1311.508789, frac_active=0.080040\n",
      "Epoch 0/10, Step 520: lr=0.001000, recon_loss=2.327636, sparsity_loss=1296.564453, frac_active=0.079189\n",
      "Epoch 0/10, Step 530: lr=0.001000, recon_loss=2.242826, sparsity_loss=1314.023438, frac_active=0.080278\n",
      "Epoch 0/10, Step 540: lr=0.001000, recon_loss=2.145016, sparsity_loss=1310.004883, frac_active=0.080017\n",
      "Epoch 0/10, Step 550: lr=0.001000, recon_loss=2.255742, sparsity_loss=1317.550781, frac_active=0.080415\n",
      "Epoch 0/10, Step 560: lr=0.001000, recon_loss=2.119081, sparsity_loss=1338.272461, frac_active=0.081842\n",
      "Epoch 0/10, Step 570: lr=0.001000, recon_loss=2.020264, sparsity_loss=1327.618164, frac_active=0.081059\n",
      "Epoch 0/10, Step 580: lr=0.001000, recon_loss=2.022569, sparsity_loss=1314.850586, frac_active=0.080324\n",
      "Epoch 0/10, Step 590: lr=0.001000, recon_loss=1.964208, sparsity_loss=1335.068359, frac_active=0.081678\n",
      "Epoch 0/10, Step 600: lr=0.001000, recon_loss=1.912446, sparsity_loss=1352.127930, frac_active=0.082591\n",
      "Epoch 0/10, Step 610: lr=0.001000, recon_loss=2.270462, sparsity_loss=1356.722656, frac_active=0.082588\n",
      "Epoch 0/10, Step 620: lr=0.001000, recon_loss=1.950876, sparsity_loss=1330.779297, frac_active=0.081315\n",
      "Epoch 0/10, Step 630: lr=0.001000, recon_loss=1.890375, sparsity_loss=1359.817383, frac_active=0.083116\n",
      "Epoch 0/10, Step 640: lr=0.001000, recon_loss=2.129074, sparsity_loss=1393.224609, frac_active=0.084936\n",
      "Epoch 0/10, Step 650: lr=0.001000, recon_loss=1.906779, sparsity_loss=1301.365234, frac_active=0.079532\n",
      "Epoch 0/10, Step 660: lr=0.001000, recon_loss=1.793908, sparsity_loss=1349.924805, frac_active=0.082616\n",
      "Epoch 0/10, Step 670: lr=0.001000, recon_loss=1.750556, sparsity_loss=1363.858398, frac_active=0.083251\n",
      "Epoch 0/10, Step 680: lr=0.001000, recon_loss=2.086347, sparsity_loss=1368.560547, frac_active=0.083125\n",
      "Epoch 0/10, Step 690: lr=0.001000, recon_loss=1.730110, sparsity_loss=1358.219727, frac_active=0.083094\n",
      "Epoch 0/10, Step 700: lr=0.001000, recon_loss=1.724593, sparsity_loss=1392.068359, frac_active=0.085135\n",
      "Epoch 0/10, Step 710: lr=0.001000, recon_loss=1.828645, sparsity_loss=1396.765625, frac_active=0.085041\n",
      "Epoch 0/10, Step 720: lr=0.001000, recon_loss=1.757867, sparsity_loss=1347.940430, frac_active=0.082367\n",
      "Epoch 0/10, Step 730: lr=0.001000, recon_loss=1.675462, sparsity_loss=1391.687500, frac_active=0.085122\n",
      "Epoch 0/10, Step 740: lr=0.001000, recon_loss=1.623930, sparsity_loss=1417.674805, frac_active=0.086608\n",
      "Epoch 0/10, Step 750: lr=0.001000, recon_loss=1.853043, sparsity_loss=1347.115234, frac_active=0.081857\n",
      "Epoch 0/10, Step 760: lr=0.001000, recon_loss=1.683420, sparsity_loss=1357.292969, frac_active=0.083215\n",
      "Epoch 0/10, Step 770: lr=0.001000, recon_loss=1.510368, sparsity_loss=1397.152344, frac_active=0.085436\n",
      "Epoch 0/10, Step 780: lr=0.001000, recon_loss=1.587073, sparsity_loss=1394.453125, frac_active=0.084987\n",
      "Epoch 0/10, Step 790: lr=0.001000, recon_loss=1.966963, sparsity_loss=1341.254883, frac_active=0.081502\n",
      "Epoch 0/10, Step 800: lr=0.001000, recon_loss=1.684028, sparsity_loss=1370.870117, frac_active=0.084036\n",
      "Epoch 0/10, Step 810: lr=0.001000, recon_loss=1.479583, sparsity_loss=1434.685547, frac_active=0.087728\n",
      "Epoch 0/10, Step 820: lr=0.001000, recon_loss=1.592360, sparsity_loss=1430.737305, frac_active=0.087255\n",
      "Epoch 0/10, Step 830: lr=0.001000, recon_loss=1.524006, sparsity_loss=1420.513672, frac_active=0.086737\n",
      "Epoch 0/10, Step 840: lr=0.001000, recon_loss=1.366923, sparsity_loss=1405.660156, frac_active=0.085967\n",
      "Epoch 0/10, Step 850: lr=0.001000, recon_loss=1.371266, sparsity_loss=1434.526367, frac_active=0.087513\n",
      "Epoch 0/10, Step 860: lr=0.001000, recon_loss=1.384916, sparsity_loss=1427.576172, frac_active=0.087126\n",
      "Epoch 0/10, Step 870: lr=0.001000, recon_loss=1.437472, sparsity_loss=1409.692383, frac_active=0.086047\n",
      "Epoch 0/10, Step 880: lr=0.001000, recon_loss=1.338798, sparsity_loss=1424.550781, frac_active=0.086999\n",
      "Epoch 0/10, Step 890: lr=0.001000, recon_loss=1.409558, sparsity_loss=1419.755859, frac_active=0.086595\n",
      "Epoch 0/10, Step 900: lr=0.001000, recon_loss=1.296586, sparsity_loss=1436.854492, frac_active=0.087728\n",
      "Epoch 0/10, Step 910: lr=0.001000, recon_loss=1.567184, sparsity_loss=1453.588867, frac_active=0.088438\n",
      "Epoch 0/10, Step 920: lr=0.001000, recon_loss=1.334982, sparsity_loss=1415.246094, frac_active=0.086461\n",
      "Epoch 0/10, Step 930: lr=0.001000, recon_loss=1.241797, sparsity_loss=1455.292969, frac_active=0.088912\n",
      "Epoch 0/10, Step 940: lr=0.001000, recon_loss=1.317149, sparsity_loss=1445.659180, frac_active=0.088198\n",
      "Epoch 0/10, Step 950: lr=0.001000, recon_loss=1.359895, sparsity_loss=1441.201172, frac_active=0.087977\n",
      "Epoch 0/10, Step 960: lr=0.001000, recon_loss=1.285498, sparsity_loss=1455.964844, frac_active=0.088984\n",
      "Epoch 0/10, Step 970: lr=0.001000, recon_loss=1.383681, sparsity_loss=1443.382812, frac_active=0.087985\n",
      "Epoch 0/10, Step 980: lr=0.001000, recon_loss=1.265652, sparsity_loss=1448.435547, frac_active=0.088482\n",
      "Epoch 0/10, Step 990: lr=0.001000, recon_loss=1.236798, sparsity_loss=1462.317383, frac_active=0.089347\n",
      "Epoch 0/10, Step 1000: lr=0.001000, recon_loss=1.524390, sparsity_loss=1410.820312, frac_active=0.085590\n",
      "Epoch 0/10, Step 1010: lr=0.001000, recon_loss=1.306528, sparsity_loss=1412.289062, frac_active=0.086424\n",
      "Epoch 0/10, Step 1020: lr=0.001000, recon_loss=1.146600, sparsity_loss=1445.123047, frac_active=0.088316\n",
      "Epoch 0/10, Step 1030: lr=0.001000, recon_loss=1.550344, sparsity_loss=1434.624023, frac_active=0.087149\n",
      "Epoch 0/10, Step 1040: lr=0.001000, recon_loss=1.209694, sparsity_loss=1400.008789, frac_active=0.085606\n",
      "Epoch 0/10, Step 1050: lr=0.001000, recon_loss=1.225187, sparsity_loss=1488.188477, frac_active=0.091005\n",
      "Epoch 0/10, Step 1060: lr=0.001000, recon_loss=1.186714, sparsity_loss=1486.708008, frac_active=0.090682\n",
      "Epoch 0/10, Step 1070: lr=0.001000, recon_loss=1.345331, sparsity_loss=1406.198242, frac_active=0.085467\n",
      "Epoch 0/10, Step 1080: lr=0.001000, recon_loss=1.230562, sparsity_loss=1439.163086, frac_active=0.088046\n",
      "Epoch 0/10, Step 1090: lr=0.001000, recon_loss=1.152950, sparsity_loss=1474.824219, frac_active=0.090012\n",
      "Epoch 0/10, Step 1100: lr=0.001000, recon_loss=1.528727, sparsity_loss=1433.374023, frac_active=0.086978\n",
      "Epoch 0/10, Step 1110: lr=0.001000, recon_loss=1.132187, sparsity_loss=1424.998047, frac_active=0.087175\n",
      "Epoch 0/10, Step 1120: lr=0.001000, recon_loss=1.103380, sparsity_loss=1473.753906, frac_active=0.090063\n",
      "Epoch 0/10, Step 1130: lr=0.001000, recon_loss=1.295341, sparsity_loss=1486.542969, frac_active=0.090476\n",
      "Epoch 0/10, Step 1140: lr=0.001000, recon_loss=1.284941, sparsity_loss=1394.441406, frac_active=0.085271\n",
      "Epoch 0/10, Step 1150: lr=0.001000, recon_loss=1.181210, sparsity_loss=1466.028320, frac_active=0.089915\n",
      "Epoch 0/10, Step 1160: lr=0.001000, recon_loss=1.020685, sparsity_loss=1469.571289, frac_active=0.089713\n",
      "Epoch 0/10, Step 1170: lr=0.001000, recon_loss=2.181075, sparsity_loss=1418.874023, frac_active=0.085553\n",
      "Epoch 0/10, Step 1180: lr=0.001000, recon_loss=1.246387, sparsity_loss=1431.863281, frac_active=0.087878\n",
      "Epoch 0/10, Step 1190: lr=0.001000, recon_loss=0.997553, sparsity_loss=1460.532227, frac_active=0.089341\n",
      "Epoch 0/10, Step 1200: lr=0.001000, recon_loss=1.108313, sparsity_loss=1507.634766, frac_active=0.091969\n",
      "Epoch 0/10, Step 1210: lr=0.001000, recon_loss=1.254029, sparsity_loss=1425.354492, frac_active=0.086687\n",
      "Epoch 0/10, Step 1220: lr=0.001000, recon_loss=1.055222, sparsity_loss=1422.661133, frac_active=0.087111\n",
      "Epoch 0/10, Step 1230: lr=0.001000, recon_loss=0.988151, sparsity_loss=1466.627930, frac_active=0.089570\n",
      "Epoch 0/10, Step 1240: lr=0.001000, recon_loss=1.226373, sparsity_loss=1495.340820, frac_active=0.090988\n",
      "Epoch 1/10, Step 1250: lr=0.001000, recon_loss=1.129412, sparsity_loss=1446.756836, frac_active=0.088304\n",
      "Epoch 1/10, Step 1260: lr=0.001000, recon_loss=0.979369, sparsity_loss=1465.012695, frac_active=0.089568\n",
      "Epoch 1/10, Step 1270: lr=0.001000, recon_loss=1.012360, sparsity_loss=1508.145508, frac_active=0.092176\n",
      "Epoch 1/10, Step 1280: lr=0.001000, recon_loss=1.192238, sparsity_loss=1465.788086, frac_active=0.089176\n",
      "Epoch 1/10, Step 1290: lr=0.001000, recon_loss=1.088681, sparsity_loss=1492.557617, frac_active=0.091209\n",
      "Epoch 1/10, Step 1300: lr=0.001000, recon_loss=0.983767, sparsity_loss=1486.364258, frac_active=0.090792\n",
      "Epoch 1/10, Step 1310: lr=0.001000, recon_loss=1.141029, sparsity_loss=1475.710938, frac_active=0.089944\n",
      "Epoch 1/10, Step 1320: lr=0.001000, recon_loss=0.986244, sparsity_loss=1451.699219, frac_active=0.088711\n",
      "Epoch 1/10, Step 1330: lr=0.001000, recon_loss=0.978496, sparsity_loss=1479.811523, frac_active=0.090376\n",
      "Epoch 1/10, Step 1340: lr=0.001000, recon_loss=1.045665, sparsity_loss=1496.412109, frac_active=0.091352\n",
      "Epoch 1/10, Step 1350: lr=0.001000, recon_loss=1.215070, sparsity_loss=1418.083984, frac_active=0.086368\n",
      "Epoch 1/10, Step 1360: lr=0.001000, recon_loss=0.959662, sparsity_loss=1454.352539, frac_active=0.089169\n",
      "Epoch 1/10, Step 1370: lr=0.001000, recon_loss=0.914787, sparsity_loss=1485.904297, frac_active=0.090718\n",
      "Epoch 1/10, Step 1380: lr=0.001000, recon_loss=1.686997, sparsity_loss=1456.605469, frac_active=0.088159\n",
      "Epoch 1/10, Step 1390: lr=0.001000, recon_loss=1.010719, sparsity_loss=1421.128906, frac_active=0.087052\n",
      "Epoch 1/10, Step 1400: lr=0.001000, recon_loss=0.871928, sparsity_loss=1475.308594, frac_active=0.090222\n",
      "Epoch 1/10, Step 1410: lr=0.001000, recon_loss=0.915763, sparsity_loss=1456.357422, frac_active=0.088875\n",
      "Epoch 1/10, Step 1420: lr=0.001000, recon_loss=1.118781, sparsity_loss=1395.272461, frac_active=0.084361\n",
      "Epoch 1/10, Step 1430: lr=0.001000, recon_loss=1.016003, sparsity_loss=1409.947266, frac_active=0.086504\n",
      "Epoch 1/10, Step 1440: lr=0.001000, recon_loss=0.887330, sparsity_loss=1492.492188, frac_active=0.091248\n",
      "Epoch 1/10, Step 1450: lr=0.001000, recon_loss=1.082982, sparsity_loss=1499.071289, frac_active=0.091404\n",
      "Epoch 1/10, Step 1460: lr=0.001000, recon_loss=1.141498, sparsity_loss=1391.177734, frac_active=0.084643\n",
      "Epoch 1/10, Step 1470: lr=0.001000, recon_loss=0.921669, sparsity_loss=1448.466797, frac_active=0.088851\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 47\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Train the model on the activations.\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# The optimize_on_activations method expects a tensor of shape (N, seq_len, d_model)\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# and will flatten the sequence dimension into the batch.\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m data_log \u001b[38;5;241m=\u001b[39m \u001b[43msae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize_on_activations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Changed from steps=steps to epochs=10 (or your desired number of epochs)\u001b[39;49;00m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Print final statistics from the training log.\u001b[39;00m\n",
      "File \u001b[0;32m~/sae-training/jumprelu_sae.py:313\u001b[0m, in \u001b[0;36mJumpReLUSAE.optimize_on_activations\u001b[0;34m(self, activations, batch_size, epochs, log_freq, lr, lr_scale, resample_method, resample_freq, resample_window, resample_scale, hidden_sample_size)\u001b[0m\n\u001b[1;32m    311\u001b[0m total_loss, loss_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(batch)\n\u001b[1;32m    312\u001b[0m \u001b[38;5;66;03m#total_loss.mean().backward()\u001b[39;00m\n\u001b[0;32m--> 313\u001b[0m \u001b[43mtotal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# Optionally perform resampling of dead latents\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sae/lib/python3.12/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sae/lib/python3.12/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sae/lib/python3.12/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
